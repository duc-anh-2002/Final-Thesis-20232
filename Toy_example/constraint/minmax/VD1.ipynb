{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "from autograd import grad\n",
    "import matplotlib.pyplot as plt\n",
    "import torch, math, random \n",
    "from pymoo.util.ref_dirs import get_reference_directions\n",
    "from pymoo.visualization.scatter import Scatter\n",
    "import os \n",
    "\n",
    "Kappa = 0.5\n",
    "k = 0\n",
    "alpha = 0\n",
    "eps = 0.4\n",
    "delta = 0.3\n",
    "num_of_component_functions = 2\n",
    "\n",
    "def J_delta(x):\n",
    "    J_delta = []\n",
    "    for i in range (1, num_of_component_functions + 1):\n",
    "        if (f(x, i) >= F(x) - delta):\n",
    "            J_delta.append(i)\n",
    "    return J_delta\n",
    "\n",
    "def f(x, i):\n",
    "    if i == 1:\n",
    "        return (1/25 * x[0]**2 + 1/100 * (x[1]-9/2)**2)\n",
    "    if i == 2:\n",
    "        return (1/25 * x[1]**2 + 1/100 * (x[0]-9/2)**2)\n",
    "     \n",
    "def g(x, i):\n",
    "    if i == 1:\n",
    "        return f(x, i) - 0.1\n",
    "    if i == 2:\n",
    "        return f(x, i) - 0.2\n",
    "\n",
    "def F(x):\n",
    "    return max(f(x, i) for i in range (1, num_of_component_functions + 1))\n",
    "\n",
    "def objective(X, x):\n",
    "    return X[0] + 1/2 * np.linalg.norm(X[1:])**2       \n",
    "\n",
    "def constraint1(X):\n",
    "    y1 = f(x, 1)\n",
    "    grad_f1 = torch.autograd.grad(y1, x, create_graph=True)\n",
    "    return -1*(np.dot(grad_f1[0].detach().numpy(), X[1:]) + y1.detach().numpy() - X[0])\n",
    "\n",
    "def constraint2(X): \n",
    "    y2 = f(x, 2)\n",
    "    grad_f2 = torch.autograd.grad(y2, x, create_graph=True)\n",
    "    # print(-1*(np.dot(grad_f2[0].detach().numpy(), X[1:]) + y2.detach().numpy() - X[0]))\n",
    "    return -1*(np.dot(grad_f2[0].detach().numpy(), X[1:]) + y2.detach().numpy() - X[0])\n",
    "\n",
    "def constraint_g1(X):\n",
    "    # print(-1 * (g(x, 1).detach().numpy()))\n",
    "    return -1 * (g(x, 1).detach().numpy())\n",
    "\n",
    "def constraint_g2(X):\n",
    "    return -1 * (g(x, 2).detach().numpy())\n",
    "\n",
    "\n",
    "def project_x(x, g):\n",
    "    \"\"\"\n",
    "    Project x into the space constrained by g(x,1) <= 0 and g(x,2) <= 0\n",
    "    \n",
    "    Parameters:\n",
    "    x (numpy.ndarray): The point to be projected\n",
    "    g (callable): The constraint function g(x, i)\n",
    "    \n",
    "    Returns:\n",
    "    numpy.ndarray: The projected point\n",
    "    \"\"\"\n",
    "    def objective(y):\n",
    "        return np.sum((y - x)**2)\n",
    "    \n",
    "    def constraints(y):\n",
    "        return np.array([g(y, 1), g(y, 2)])\n",
    "    \n",
    "    cons = {'type': 'ineq', 'fun': lambda y: -constraints(y)}\n",
    "    \n",
    "    result = minimize(objective, x, method='SLSQP', constraints=cons, tol=1e-20)\n",
    "    \n",
    "    return result.x\n",
    "\n",
    "def solve_p(X, x): \n",
    "    k_delta = J_delta(x) \n",
    "    constraint = []\n",
    "    if 1 in k_delta:\n",
    "        constraint.append({'type':'ineq', 'fun': constraint1})\n",
    "    if 2 in k_delta:\n",
    "        constraint.append({'type':'ineq', 'fun': constraint2})\n",
    "    constraint.append({'type':'ineq', 'fun': constraint_g1})\n",
    "    constraint.append({'type':'ineq', 'fun': constraint_g2})\n",
    "    res = minimize(objective, x0= X, args = (x), method='SLSQP', constraints=constraint, tol=1e-20)\n",
    "    return res.x \n",
    "\n",
    "\n",
    "def gradient_1(X, x):\n",
    "    Kappa_new = 1\n",
    "   \n",
    "    while (F(x.detach().numpy() + Kappa_new * np.array(X[1:])) > F(x.detach().numpy()) - Kappa_new * eps * np.linalg.norm(X[1:])**2):\n",
    "        Kappa_new = Kappa_new * Kappa\n",
    "    x_new = x.detach().numpy() + Kappa_new * np.array(X[1:])\n",
    "    # print('kappa = ', Kappa_new)\n",
    "    x_new = project_x(x_new, g)\n",
    "    return x_new\n",
    "# def gradient_1(X, x):\n",
    "#     Kappa_new = 1\n",
    "   \n",
    "#     while (F(x.detach().numpy() + Kappa_new * np.array(X[1:])) > F(x.detach().numpy()) - Kappa_new * eps * np.linalg.norm(X[1:])**2):\n",
    "#         Kappa_new = Kappa_new * Kappa\n",
    "#     x_new = x.detach().numpy() + Kappa_new * np.array(X[1:])\n",
    "#     # print('kappa = ', Kappa_new)\n",
    "#     x_new = project_x(x_new, g)\n",
    "#     return x_new\n",
    "\n",
    "def gradient(X, x, kappa, sigma, eta, iter):\n",
    "    x_new = x.detach().numpy() + kappa * np.array(X[1:])\n",
    "    if (F(x.detach().numpy() + kappa * np.array(X[1:])) <= F(x.detach().numpy()) - kappa * eps * np.linalg.norm(X[1:])**2):\n",
    "        kappa =  kappa + eta ** iter\n",
    "    else:\n",
    "        kappa = kappa * sigma\n",
    "    return x_new, kappa\n",
    "\n",
    "def f1(x):\n",
    "    #2 dims\n",
    "    return 1/25*x[0]**2+1/100*(x[1]-9/2)**2\n",
    "def f2(x):\n",
    "    return 1/25*x[1]**2+1/100*(x[0]-9/2)**2\n",
    "\n",
    "\n",
    "def create_pf1():\n",
    "    ps1 = np.linspace(-6, 0, num=500)\n",
    "    pf = []\n",
    "    for x1 in ps1:\n",
    "        x = [9*x1/(2*x1-8),9/(2-8*x1)]\n",
    "        f = [f1(x), f2(x)]\n",
    "        pf.append(f)\n",
    "    pf = np.array(pf)\n",
    "    return pf\n",
    "\n",
    "def bbox(min_f1, max_f1, min_f2, max_f2):  \n",
    "    # Draw the bounding box based on the constrained min/max values\n",
    "    plt.gca().add_patch(plt.Rectangle((min_f1, min_f2), max_f1 - min_f1, max_f2 - min_f2, \n",
    "                                      fill=False, edgecolor='blue', linewidth=2, linestyle=\"--\"))\n",
    "    print(f\"BBox: f1 in [{min_f1}, {max_f1}], f2 in [{min_f2}, {max_f2}]\")\n",
    "\n",
    "def get_scaled_reference_directions(num_partitions, min_f1, max_f1, min_f2, max_f2):\n",
    "    # Get the normalized reference directions\n",
    "    test_rays = get_reference_directions(\"das-dennis\", 2, n_partitions=num_partitions).astype(np.float32)\n",
    "    \n",
    "    # Scale the reference directions to fit within the bounding box\n",
    "    f1_scaled = test_rays[:, 0] * (max_f1 - min_f1) + min_f1\n",
    "    f2_scaled = test_rays[:, 1] * (max_f2 - min_f2) + min_f2\n",
    "    \n",
    "    # Combine the scaled directions into a single array\n",
    "    scaled_ref_dirs = np.column_stack((f1_scaled, f2_scaled))\n",
    "    \n",
    "    return scaled_ref_dirs\n",
    "import time\n",
    "if __name__ == \"__main__\":\n",
    "    test_rays = get_reference_directions(\"das-dennis\", 2, n_partitions=10).astype(\n",
    "    np.float32)\n",
    "    number_of_iteration = 3\n",
    "    b = 2.0\n",
    "    p = [-1.0, -1.0]\n",
    "    X = [b, *p]\n",
    "    res = []  \n",
    "    # non-monotone adaptive\n",
    "    sigma = 0.9\n",
    "    eta = 0.01\n",
    "    # constraint\n",
    "    # min_f1, max_f1, min_f2, max_f2 = 0.08, 0.3, 0.12, 0.2\n",
    "    min_f1, max_f1, min_f2, max_f2 = 0.08, 0.3, 0.08, 0.3\n",
    "    ref = []\n",
    "    ref = get_scaled_reference_directions(10, min_f1, max_f1, min_f2, max_f2)\n",
    "    for i in range (10):\n",
    "        # x0 = [float(random.randrange(-10.0, 10.0)), float(random.randrange(-10.0, 10.0))]\n",
    "        x0 = [-1.0, -1.0]\n",
    "        x = torch.tensor(x0, requires_grad=True) \n",
    "        # print(x)\n",
    "        # kappa = 1\n",
    "        start_time = time.time()\n",
    "        for iter in range (number_of_iteration):\n",
    "            X_new = solve_p(X, x)\n",
    "            x_new = gradient_1(X_new, x) \n",
    "            # print(f(x_new, 1), f(x_new, 2))\n",
    "            x = torch.tensor(x_new, requires_grad=True)\n",
    "            X = [F(x.detach().numpy()), *X_new[1:]]\n",
    "        end_time = time.time()\n",
    "        print(f\"Time taken for iteration {i}: {end_time - start_time} seconds\")\n",
    "        res.append([f(x_new, 1), f(x_new, 2)])\n",
    "    print(x_new)\n",
    "    print(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w.o. constraint and adaptive projection [Thang et al., 2024]\n",
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "from autograd import grad\n",
    "import matplotlib.pyplot as plt\n",
    "import torch, math, random \n",
    "from pymoo.util.ref_dirs import get_reference_directions\n",
    "from pymoo.visualization.scatter import Scatter\n",
    "import os \n",
    "\n",
    "Kappa = 0.5\n",
    "k = 0\n",
    "alpha = 0\n",
    "eps = 0.4\n",
    "delta = 0.3\n",
    "num_of_component_functions = 2\n",
    "\n",
    "def J_delta(x):\n",
    "    J_delta = []\n",
    "    for i in range (1, num_of_component_functions + 1):\n",
    "        if (f(x, i) >= F(x) - delta):\n",
    "            J_delta.append(i)\n",
    "    return J_delta\n",
    "\n",
    "def f(x, i):\n",
    "    if i == 1:\n",
    "        return (1/25 * x[0]**2 + 1/100 * (x[1]-9/2)**2)\n",
    "    if i == 2:\n",
    "        return (1/25 * x[1]**2 + 1/100 * (x[0]-9/2)**2)\n",
    "     \n",
    "def g(x, i):\n",
    "    if i == 1:\n",
    "        return f(x, i) - 0.1\n",
    "    if i == 2:\n",
    "        return f(x, i) - 0.2\n",
    "\n",
    "def F(x):\n",
    "    return max(f(x, i) for i in range (1, num_of_component_functions + 1))\n",
    "\n",
    "def objective(X, x):\n",
    "    return X[0] + 1/2 * np.linalg.norm(X[1:])**2       \n",
    "\n",
    "def constraint1(X):\n",
    "    y1 = f(x, 1)\n",
    "    grad_f1 = torch.autograd.grad(y1, x, create_graph=True)\n",
    "    return -1*(np.dot(grad_f1[0].detach().numpy(), X[1:]) + y1.detach().numpy() - X[0])\n",
    "\n",
    "def constraint2(X): \n",
    "    y2 = f(x, 2)\n",
    "    grad_f2 = torch.autograd.grad(y2, x, create_graph=True)\n",
    "    # print(-1*(np.dot(grad_f2[0].detach().numpy(), X[1:]) + y2.detach().numpy() - X[0]))\n",
    "    return -1*(np.dot(grad_f2[0].detach().numpy(), X[1:]) + y2.detach().numpy() - X[0])\n",
    "\n",
    "def constraint_g1(X):\n",
    "    # print(-1 * (g(x, 1).detach().numpy()))\n",
    "    return -1 * (g(x, 1).detach().numpy())\n",
    "\n",
    "def constraint_g2(X):\n",
    "    return -1 * (g(x, 2).detach().numpy())\n",
    "\n",
    "\n",
    "def project_x(x, g):\n",
    "    \"\"\"\n",
    "    Project x into the space constrained by g(x,1) <= 0 and g(x,2) <= 0\n",
    "    \n",
    "    Parameters:\n",
    "    x (numpy.ndarray): The point to be projected\n",
    "    g (callable): The constraint function g(x, i)\n",
    "    \n",
    "    Returns:\n",
    "    numpy.ndarray: The projected point\n",
    "    \"\"\"\n",
    "    def objective(y):\n",
    "        return np.sum((y - x)**2)\n",
    "    \n",
    "    def constraints(y):\n",
    "        return np.array([g(y, 1), g(y, 2)])\n",
    "    \n",
    "    cons = {'type': 'ineq', 'fun': lambda y: -constraints(y)}\n",
    "    \n",
    "    result = minimize(objective, x, method='SLSQP', constraints=cons, tol=1e-20)\n",
    "    \n",
    "    return result.x\n",
    "\n",
    "def solve_p(X, x): \n",
    "    k_delta = J_delta(x) \n",
    "    constraint = []\n",
    "    if 1 in k_delta:\n",
    "        constraint.append({'type':'ineq', 'fun': constraint1})\n",
    "    if 2 in k_delta:\n",
    "        constraint.append({'type':'ineq', 'fun': constraint2})\n",
    "    constraint.append({'type':'ineq', 'fun': constraint_g1})\n",
    "    constraint.append({'type':'ineq', 'fun': constraint_g2})\n",
    "    res = minimize(objective, x0= X, args = (x), method='SLSQP', constraints=constraint, tol=1e-20)\n",
    "    return res.x \n",
    "\n",
    "\n",
    "def gradient_1(X, x, kappa, sigma):\n",
    "    x_new = x.detach().numpy() + kappa * np.array(X[1:])\n",
    "    x_new = project_x(x_new, g)\n",
    "    if (F(x_new) <= F(x.detach().numpy()) - kappa * eps * np.linalg.norm(X[1:])**2):\n",
    "        kappa = kappa\n",
    "    else:\n",
    "        kappa = kappa * sigma\n",
    "    return x_new, kappa\n",
    "\n",
    "def gradient(X, x, kappa, sigma, eta, iter):\n",
    "    x_new = x.detach().numpy() + kappa * np.array(X[1:])\n",
    "    if (F(x.detach().numpy() + kappa * np.array(X[1:])) <= F(x.detach().numpy()) - kappa * eps * np.linalg.norm(X[1:])**2):\n",
    "        kappa =  kappa + eta ** iter\n",
    "    else:\n",
    "        kappa = kappa * sigma\n",
    "    return x_new, kappa\n",
    "\n",
    "def f1(x):\n",
    "    #2 dims\n",
    "    return 1/25*x[0]**2+1/100*(x[1]-9/2)**2\n",
    "def f2(x):\n",
    "    return 1/25*x[1]**2+1/100*(x[0]-9/2)**2\n",
    "\n",
    "\n",
    "def create_pf1():\n",
    "    ps1 = np.linspace(-6, 0, num=500)\n",
    "    pf = []\n",
    "    for x1 in ps1:\n",
    "        x = [9*x1/(2*x1-8),9/(2-8*x1)]\n",
    "        f = [f1(x), f2(x)]\n",
    "        pf.append(f)\n",
    "    pf = np.array(pf)\n",
    "    return pf\n",
    "\n",
    "def bbox(min_f1, max_f1, min_f2, max_f2):  \n",
    "    # Draw the bounding box based on the constrained min/max values\n",
    "    plt.gca().add_patch(plt.Rectangle((min_f1, min_f2), max_f1 - min_f1, max_f2 - min_f2, \n",
    "                                      fill=False, edgecolor='blue', linewidth=2, linestyle=\"--\"))\n",
    "    print(f\"BBox: f1 in [{min_f1}, {max_f1}], f2 in [{min_f2}, {max_f2}]\")\n",
    "\n",
    "def get_scaled_reference_directions(num_partitions, min_f1, max_f1, min_f2, max_f2):\n",
    "    # Get the normalized reference directions\n",
    "    test_rays = get_reference_directions(\"das-dennis\", 2, n_partitions=num_partitions).astype(np.float32)\n",
    "    \n",
    "    # Scale the reference directions to fit within the bounding box\n",
    "    f1_scaled = test_rays[:, 0] * (max_f1 - min_f1) + min_f1\n",
    "    f2_scaled = test_rays[:, 1] * (max_f2 - min_f2) + min_f2\n",
    "    \n",
    "    # Combine the scaled directions into a single array\n",
    "    scaled_ref_dirs = np.column_stack((f1_scaled, f2_scaled))\n",
    "    \n",
    "    return scaled_ref_dirs\n",
    "import time\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_rays = get_reference_directions(\"das-dennis\", 2, n_partitions=10).astype(\n",
    "    np.float32)\n",
    "    number_of_iteration = 10\n",
    "    b = 2.0\n",
    "    p = [-1.0, -1.0]\n",
    "    X = [b, *p]\n",
    "    res = []  \n",
    "    # non-monotone adaptive\n",
    "    sigma = 0.9\n",
    "    eta = 0.01\n",
    "    # constraint\n",
    "    # min_f1, max_f1, min_f2, max_f2 = 0.08, 0.3, 0.12, 0.2\n",
    "    min_f1, max_f1, min_f2, max_f2 = 0.08, 0.3, 0.08, 0.3\n",
    "    ref = []\n",
    "    ref = get_scaled_reference_directions(10, min_f1, max_f1, min_f2, max_f2)\n",
    "    for i in range (10):\n",
    "        x0 = [float(random.randrange(-10.0, 10.0)), float(random.randrange(-10.0, 10.0))]\n",
    "        # x0 = [-1.0, -1.0]\n",
    "        x = torch.tensor(x0, requires_grad=True) \n",
    "        # print(x)\n",
    "        kappa = 1\n",
    "        start_time = time.time()\n",
    "        for iter in range (number_of_iteration):\n",
    "            X_new = solve_p(X, x)\n",
    "            x_new, kappa = gradient_1(X_new, x, kappa, sigma=0.95) \n",
    "            x = torch.tensor(x_new, requires_grad=True)\n",
    "            X = [F(x.detach().numpy()), *X_new[1:]]\n",
    "        end_time = time.time()\n",
    "        print(f\"Time taken for sample {i}: {end_time - start_time} seconds\")\n",
    "        res.append([f(x_new, 1), f(x_new, 2)])\n",
    "    print(x_new)\n",
    "    print(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "from autograd import grad\n",
    "import matplotlib.pyplot as plt\n",
    "import torch, math, random \n",
    "from pymoo.util.ref_dirs import get_reference_directions\n",
    "from pymoo.visualization.scatter import Scatter\n",
    "import os \n",
    "\n",
    "Kappa = 0.5\n",
    "k = 0\n",
    "alpha = 0\n",
    "eps = 0.4\n",
    "delta = 0.3\n",
    "num_of_component_functions = 2\n",
    "\n",
    "def J_delta(x):\n",
    "    J_delta = []\n",
    "    for i in range (1, num_of_component_functions + 1):\n",
    "        if (f(x, i) >= F(x) - delta):\n",
    "            J_delta.append(i)\n",
    "    return J_delta\n",
    "\n",
    "\n",
    "def J_h(x):\n",
    "    J_h = []\n",
    "    for i in range (1, num_of_component_functions + 1):\n",
    "        if (g(x, i) == 0):\n",
    "            J_h.append(i)\n",
    "    return J_h\n",
    "\n",
    "def f(x, i):\n",
    "    if i == 1:\n",
    "        return (1/25 * x[0]**2 + 1/100 * (x[1]-9/2)**2)\n",
    "    if i == 2:\n",
    "        return (1/25 * x[1]**2 + 1/100 * (x[0]-9/2)**2)\n",
    "     \n",
    "def g(x, i):\n",
    "    if i == 1:\n",
    "        return f(x, i) - 0.1\n",
    "    if i == 2:\n",
    "        return f(x, i) - 0.2\n",
    "\n",
    "def F(x):\n",
    "    return max(f(x, i) for i in range (1, num_of_component_functions + 1))\n",
    "\n",
    "def objective(X, x):\n",
    "    return X[0] + 1/2 * np.linalg.norm(X[1:])**2       \n",
    "\n",
    "def constraint1(X):\n",
    "    y1 = f(x, 1)\n",
    "    grad_f1 = torch.autograd.grad(y1, x, create_graph=True)\n",
    "    return -1*(np.dot(grad_f1[0].detach().numpy(), X[1:]) + y1.detach().numpy() - X[0])\n",
    "\n",
    "def constraint2(X): \n",
    "    y2 = f(x, 2)\n",
    "    grad_f2 = torch.autograd.grad(y2, x, create_graph=True)\n",
    "    # print(-1*(np.dot(grad_f2[0].detach().numpy(), X[1:]) + y2.detach().numpy() - X[0]))\n",
    "    return -1*(np.dot(grad_f2[0].detach().numpy(), X[1:]) + y2.detach().numpy() - X[0])\n",
    "\n",
    "def constraint_g1(X):\n",
    "    # print(-1 * (g(x, 1).detach().numpy()))\n",
    "    return -1 * (g(x, 1).detach().numpy())\n",
    "\n",
    "def constraint_g2(X):\n",
    "    return -1 * (g(x, 2).detach().numpy())\n",
    "\n",
    "\n",
    "def project_x(x, g):\n",
    "    \"\"\"\n",
    "    Project x into the space constrained by g(x,1) <= 0 and g(x,2) <= 0\n",
    "    \n",
    "    Parameters:\n",
    "    x (numpy.ndarray): The point to be projected\n",
    "    g (callable): The constraint function g(x, i)\n",
    "    \n",
    "    Returns:\n",
    "    numpy.ndarray: The projected point\n",
    "    \"\"\"\n",
    "    def objective(y):\n",
    "        return np.sum((y - x)**2)\n",
    "    \n",
    "    def constraints(y):\n",
    "        return np.array([g(y, 1), g(y, 2)])\n",
    "    \n",
    "    cons = {'type': 'ineq', 'fun': lambda y: -constraints(y)}\n",
    "    \n",
    "    result = minimize(objective, x, method='SLSQP', constraints=cons, tol=1e-20)\n",
    "    \n",
    "    return result.x\n",
    "\n",
    "\n",
    "def solve_p(X, x): \n",
    "    k_delta = J_delta(x) \n",
    "    l_delta = J_h(x)\n",
    "    constraint = []\n",
    "    if 1 in k_delta:\n",
    "        constraint.append({'type':'ineq', 'fun': constraint1})\n",
    "    if 2 in k_delta:\n",
    "        constraint.append({'type':'ineq', 'fun': constraint2})\n",
    "    if 1 in l_delta:\n",
    "        constraint.append({'type':'eq', 'fun': constraint_g1})\n",
    "    if 2 in l_delta:\n",
    "        constraint.append({'type':'eq', 'fun': constraint_g2})\n",
    "    # constraint.append({'type':'ineq', 'fun': constraint_g1})\n",
    "    # constraint.append({'type':'ineq', 'fun': constraint_g2})\n",
    "    res = minimize(objective, x0= X, args = (x), method='SLSQP', constraints=constraint, tol=1e-6)\n",
    "    return res.x \n",
    "\n",
    "\n",
    "def gradient_1(X, x, kappa, sigma, eta, iter):\n",
    "    x_new = x.detach().numpy() + kappa * np.array(X[1:])\n",
    "    x_new = project_x(x_new, g)\n",
    "    if (F(x_new) <= F(x.detach().numpy()) - kappa * eps * np.linalg.norm(X[1:])**2):\n",
    "        kappa = kappa + eta ** iter\n",
    "    else:\n",
    "        kappa = kappa * sigma\n",
    "    return x_new, kappa\n",
    "\n",
    "def gradient(X, x, kappa, sigma, eta, iter):\n",
    "    x_new = x.detach().numpy() + kappa * np.array(X[1:])\n",
    "    if (F(x.detach().numpy() + kappa * np.array(X[1:])) <= F(x.detach().numpy()) - kappa * eps * np.linalg.norm(X[1:])**2):\n",
    "        kappa =  kappa + eta ** iter\n",
    "    else:\n",
    "        kappa = kappa * sigma\n",
    "    return x_new, kappa\n",
    "\n",
    "def f1(x):\n",
    "    #2 dims\n",
    "    return 1/25*x[0]**2+1/100*(x[1]-9/2)**2\n",
    "def f2(x):\n",
    "    return 1/25*x[1]**2+1/100*(x[0]-9/2)**2\n",
    "\n",
    "\n",
    "def create_pf1():\n",
    "    ps1 = np.linspace(-6, 0, num=500)\n",
    "    pf = []\n",
    "    for x1 in ps1:\n",
    "        x = [9*x1/(2*x1-8),9/(2-8*x1)]\n",
    "        f = [f1(x), f2(x)]\n",
    "        pf.append(f)\n",
    "    pf = np.array(pf)\n",
    "    return pf\n",
    "\n",
    "def bbox(min_f1, max_f1, min_f2, max_f2):  \n",
    "    # Draw the bounding box based on the constrained min/max values\n",
    "    plt.gca().add_patch(plt.Rectangle((min_f1, min_f2), max_f1 - min_f1, max_f2 - min_f2, \n",
    "                                      fill=False, edgecolor='blue', linewidth=2, linestyle=\"--\"))\n",
    "    print(f\"BBox: f1 in [{min_f1}, {max_f1}], f2 in [{min_f2}, {max_f2}]\")\n",
    "\n",
    "def get_scaled_reference_directions(num_partitions, min_f1, max_f1, min_f2, max_f2):\n",
    "    # Get the normalized reference directions\n",
    "    test_rays = get_reference_directions(\"das-dennis\", 2, n_partitions=num_partitions).astype(np.float32)\n",
    "    \n",
    "    # Scale the reference directions to fit within the bounding box\n",
    "    f1_scaled = test_rays[:, 0] * (max_f1 - min_f1) + min_f1\n",
    "    f2_scaled = test_rays[:, 1] * (max_f2 - min_f2) + min_f2\n",
    "    \n",
    "    # Combine the scaled directions into a single array\n",
    "    scaled_ref_dirs = np.column_stack((f1_scaled, f2_scaled))\n",
    "    \n",
    "    return scaled_ref_dirs\n",
    "import time\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_rays = get_reference_directions(\"das-dennis\", 2, n_partitions=10).astype(\n",
    "    np.float32)\n",
    "    number_of_iteration = 10\n",
    "    b = 2.0\n",
    "    p = [-1.0, -1.0]\n",
    "    X = [b, *p]\n",
    "    res = []  \n",
    "    # non-monotone adaptive\n",
    "    sigma = 0.9\n",
    "    eta = 0.01\n",
    "    # constraint\n",
    "    # min_f1, max_f1, min_f2, max_f2 = 0.08, 0.3, 0.12, 0.2\n",
    "    min_f1, max_f1, min_f2, max_f2 = 0.08, 0.3, 0.08, 0.3\n",
    "    ref = []\n",
    "    ref = get_scaled_reference_directions(10, min_f1, max_f1, min_f2, max_f2)\n",
    "    for i in range (10):\n",
    "        x0 = [float(random.randrange(-10.0, 10.0)), float(random.randrange(-10.0, 10.0))]\n",
    "        # x0 = [-1.0, -1.0]\n",
    "        x = torch.tensor(x0, requires_grad=True) \n",
    "        # print(x)\n",
    "        kappa = 1\n",
    "        start_time = time.time()\n",
    "        for iter in range (number_of_iteration):\n",
    "            X_new = solve_p(X, x)\n",
    "            x_new, kappa = gradient_1(X_new, x, kappa, sigma=0.95, eta=0.1, iter=iter) \n",
    "            x = torch.tensor(x_new, requires_grad=True)\n",
    "            X = [F(x.detach().numpy()), *X_new[1:]]\n",
    "        end_time = time.time()\n",
    "        print(f\"Time taken for sample {i}: {end_time - start_time} seconds\")\n",
    "        res.append([f(x_new, 1), f(x_new, 2)])\n",
    "    print(x_new)\n",
    "    print(res)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
